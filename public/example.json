{
    "entry_id": "http://arxiv.org/abs/1709.06308v1",
    "updated": "2017-09-19T09:19:08+00:00",
    "published": "2017-09-19T09:19:08+00:00",
    "title": "Exploring Human-like Attention Supervision in Visual Question Answering",
    "authors": [
        {
            "name": "Tingting Qiao"
        },
        {
            "name": "Jianfeng Dong"
        },
        {
            "name": "Duanqing Xu"
        }
    ],
    "summary": "Attention mechanisms have been widely applied in the Visual Question\nAnswering (VQA) task, as they help to focus on the area-of-interest of both\nvisual and textual information. To answer the questions correctly, the model\nneeds to selectively target different areas of an image, which suggests that an\nattention-based model may benefit from an explicit attention supervision. In\nthis work, we aim to address the problem of adding attention supervision to VQA\nmodels. Since there is a lack of human attention data, we first propose a Human\nAttention Network (HAN) to generate human-like attention maps, training on a\nrecently released dataset called Human ATtention Dataset (VQA-HAT). Then, we\napply the pre-trained HAN on the VQA v2.0 dataset to automatically produce the\nhuman-like attention maps for all image-question pairs. The generated\nhuman-like attention map dataset for the VQA v2.0 dataset is named as\nHuman-Like ATtention (HLAT) dataset. Finally, we apply human-like attention\nsupervision to an attention-based VQA model. The experiments show that adding\nhuman-like supervision yields a more accurate attention together with a better\nperformance, showing a promising future for human-like attention supervision in\nVQA.",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "primary_category": "cs.CV",
    "categories": [
        "cs.CV"
    ],
    "links": [
        {
            "href": "http://arxiv.org/abs/1709.06308v1",
            "title": null,
            "rel": "alternate",
            "content_type": null
        },
        {
            "href": "http://arxiv.org/pdf/1709.06308v1",
            "title": "pdf",
            "rel": "related",
            "content_type": null
        }
    ],
    "pdf_url": "http://arxiv.org/pdf/1709.06308v1",
    "_raw": {
        "id": "http://arxiv.org/abs/1709.06308v1",
        "guidislink": true,
        "link": "http://arxiv.org/abs/1709.06308v1",
        "updated": "2017-09-19T09:19:08Z",
        "updated_parsed": [
            2017,
            9,
            19,
            9,
            19,
            8,
            1,
            262,
            0
        ],
        "published": "2017-09-19T09:19:08Z",
        "published_parsed": [
            2017,
            9,
            19,
            9,
            19,
            8,
            1,
            262,
            0
        ],
        "title": "Exploring Human-like Attention Supervision in Visual Question Answering",
        "title_detail": {
            "type": "text/plain",
            "language": null,
            "base": "http://export.arxiv.org/api/query?search_query=attention&id_list=&sortBy=relevance&sortOrder=descending&start=0&max_results=5",
            "value": "Exploring Human-like Attention Supervision in Visual Question Answering"
        },
        "summary": "Attention mechanisms have been widely applied in the Visual Question\nAnswering (VQA) task, as they help to focus on the area-of-interest of both\nvisual and textual information. To answer the questions correctly, the model\nneeds to selectively target different areas of an image, which suggests that an\nattention-based model may benefit from an explicit attention supervision. In\nthis work, we aim to address the problem of adding attention supervision to VQA\nmodels. Since there is a lack of human attention data, we first propose a Human\nAttention Network (HAN) to generate human-like attention maps, training on a\nrecently released dataset called Human ATtention Dataset (VQA-HAT). Then, we\napply the pre-trained HAN on the VQA v2.0 dataset to automatically produce the\nhuman-like attention maps for all image-question pairs. The generated\nhuman-like attention map dataset for the VQA v2.0 dataset is named as\nHuman-Like ATtention (HLAT) dataset. Finally, we apply human-like attention\nsupervision to an attention-based VQA model. The experiments show that adding\nhuman-like supervision yields a more accurate attention together with a better\nperformance, showing a promising future for human-like attention supervision in\nVQA.",
        "summary_detail": {
            "type": "text/plain",
            "language": null,
            "base": "http://export.arxiv.org/api/query?search_query=attention&id_list=&sortBy=relevance&sortOrder=descending&start=0&max_results=5",
            "value": "Attention mechanisms have been widely applied in the Visual Question\nAnswering (VQA) task, as they help to focus on the area-of-interest of both\nvisual and textual information. To answer the questions correctly, the model\nneeds to selectively target different areas of an image, which suggests that an\nattention-based model may benefit from an explicit attention supervision. In\nthis work, we aim to address the problem of adding attention supervision to VQA\nmodels. Since there is a lack of human attention data, we first propose a Human\nAttention Network (HAN) to generate human-like attention maps, training on a\nrecently released dataset called Human ATtention Dataset (VQA-HAT). Then, we\napply the pre-trained HAN on the VQA v2.0 dataset to automatically produce the\nhuman-like attention maps for all image-question pairs. The generated\nhuman-like attention map dataset for the VQA v2.0 dataset is named as\nHuman-Like ATtention (HLAT) dataset. Finally, we apply human-like attention\nsupervision to an attention-based VQA model. The experiments show that adding\nhuman-like supervision yields a more accurate attention together with a better\nperformance, showing a promising future for human-like attention supervision in\nVQA."
        },
        "authors": [
            {
                "name": "Tingting Qiao"
            },
            {
                "name": "Jianfeng Dong"
            },
            {
                "name": "Duanqing Xu"
            }
        ],
        "author_detail": {
            "name": "Duanqing Xu"
        },
        "author": "Duanqing Xu",
        "links": [
            {
                "href": "http://arxiv.org/abs/1709.06308v1",
                "rel": "alternate",
                "type": "text/html"
            },
            {
                "title": "pdf",
                "href": "http://arxiv.org/pdf/1709.06308v1",
                "rel": "related",
                "type": "application/pdf"
            }
        ],
        "arxiv_primary_category": {
            "term": "cs.CV",
            "scheme": "http://arxiv.org/schemas/atom"
        },
        "tags": [
            {
                "term": "cs.CV",
                "scheme": "http://arxiv.org/schemas/atom",
                "label": null
            }
        ]
    }
}